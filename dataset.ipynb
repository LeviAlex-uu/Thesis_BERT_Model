{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from random import randint\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def single_dataset(file, dir=\"data/\", lim=0):\n",
    "    \"\"\"\n",
    "    Reads file given, removes unnecessary columns and labels data.\n",
    "    file: name of .csv file to be read\n",
    "    dir: directory of the file \n",
    "    lim: limit of the data entries returned\n",
    "    \"\"\"\n",
    "    with open(dir + file, encoding=\"utf-8\") as f:\n",
    "        lines = [line[:-1] for line in f.readlines()]\n",
    "    header = lines[0].split(',')\n",
    "    clean = len(header) == 2 or file == 'test.csv'\n",
    "    \n",
    "    df = pd.read_csv(dir + file)\n",
    "    if not clean: \n",
    "        for head in header:\n",
    "            if head == 'score' or head == 'caption':\n",
    "                pass\n",
    "            else:\n",
    "                df.pop(head)\n",
    "    if lim > 0:\n",
    "        df.sort_values(by='score', ascending=False)\n",
    "        labeled_df = label_data(df)\n",
    "        return labeled_df[:lim]\n",
    "    else:\n",
    "        labeled_df = label_data(df)\n",
    "        return labeled_df   \n",
    "\n",
    "def complete_dataset():\n",
    "    \"\"\"\n",
    "    combine all excel-files to create dataset\n",
    "    \"\"\"\n",
    "    dir = \"data/\"\n",
    "    ds  = pd.read_csv(\"data/test.csv\")\n",
    "    \n",
    "    for file in os.listdir(dir):\n",
    "        df = single_dataset(file, dir=dir)\n",
    "        ds = pd.concat([ds,df], ignore_index=True)\n",
    "        \n",
    "    ds.to_csv('dataset/filename.csv')   \n",
    "    \n",
    "def get_data(file):\n",
    "    \"\"\"\n",
    "    Load dataset from a specified file\n",
    "    file: name of .csv file to be read\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"dataset/\" + file) \n",
    "    return df\n",
    "    \n",
    "def label_data(df):\n",
    "    \"\"\"\n",
    "    label the scores as 'funny' or 'unfunny'\n",
    "    df: pandas dataframe\n",
    "    \"\"\"\n",
    "    df['score'] = df['score'].apply(lambda x: 1 if x >= 2.0 else 0)\n",
    "    df['label'] = pd.DataFrame(df['score'].apply(lambda x: 'funny' if x == 1 else 'unfunny'))\n",
    "    return df \n",
    "    \n",
    "    \n",
    "def data_distribution(df):\n",
    "    \"\"\"\n",
    "    Shows how the data is distributed in funny and unfunny captions\n",
    "    df: pandas dataframe\n",
    "    \"\"\"\n",
    "    num = df['label'].value_counts()\n",
    "    if \"funny\" in num:\n",
    "        fun = round(num['funny']/len(df)*100, 2)\n",
    "        print(\"Amount of funny captions:\\t\\t\", num['funny'], \"\\t\", fun, \"%\")\n",
    "        \n",
    "    if \"unfunny\" in num:\n",
    "        nfun = round(num['unfunny']/len(df)*100, 2)\n",
    "        print(\"Amount of unfunny captions:\\t\\t\", num['unfunny'], \"\\t\", nfun, \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = single_dataset('691_summary_KLUCB.csv')\n",
    "df[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augmentMyData(df, augmenter, repetitions=1, samples=200):\n",
    "    \"\"\"\n",
    "    AugmentMyData is the implementation of the SMOTE technique, \n",
    "    it creates synthetic data points using the bert-base-uncased word embedder as augmenter.\n",
    "    df: pandas dataframe\n",
    "    augmenter: augmenter, contextual word embedder\n",
    "    repetitions: times the augmentation process is repeated\n",
    "    samples: amount of data points created in one repetition.\n",
    "    This function is written using code found here:\n",
    "    https://github.com/theartificialguy/NLP-with-Deep-Learning/blob/master/PREPROCESSING%20TECHNIQUES/spam_handling_imbalanced_data.ipynb\n",
    "    \"\"\"\n",
    "    augmented_texts = []\n",
    "    # select only the minority class samples\n",
    "    spam_df = df[df['score'] == 1].reset_index(drop=True) # removes unecessary index column\n",
    "    for i in tqdm(np.random.randint(0, len(spam_df), samples)):\n",
    "        # generating 'n_samples' augmented texts\n",
    "        for _ in range(repetitions):\n",
    "            augmented_text = augmenter.augment(spam_df['caption'].iloc[i])\n",
    "            augmented_texts.append(augmented_text)\n",
    "    \n",
    "    data = {\n",
    "        'score': 1,\n",
    "        'caption': augmented_texts,\n",
    "        'label': 'funny'\n",
    "    }\n",
    "    aug_df = pd.DataFrame(data)\n",
    "    df = shuffle(df.append(aug_df).reset_index(drop=True))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#print(torch. __version__) \n",
    "import nlpaug.augmenter.word.context_word_embs as aug\n",
    "\n",
    "augmenter = aug.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\") #BERT-base-uncased word embedding augmenter\n",
    "\n",
    "df2 = get_data('data_500_label.csv')\n",
    "sample = df2.sample(frac=0.1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (len(sample[sample['score'] == 0]) - len(sample[sample['score'] == 1]))\n",
    "df2_aug = augmentMyData(sample, augmenter, samples=diff) #augment data !!! takes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_aug.to_csv('dataset/data_500_label_SMOTE.csv') #write augmented data to file test_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(df):\n",
    "    \"\"\"\n",
    "    randomly oversamples dataset.\n",
    "    df: pandas dataframe\n",
    "    \"\"\"\n",
    "    x = df[df.score == 1]\n",
    "    y = df[df.score == 0]\n",
    "    i = len(x)\n",
    "    \n",
    "    while (i < len(y)):\n",
    "        rand = randint(0,len(x)-1)\n",
    "        df = df.append(x.iloc[rand]).reset_index(drop=True)\n",
    "        i += 1\n",
    "    res = shuffle(df)\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oversample = oversample(sample)\n",
    "data_distribution(df_oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oversample.to_csv('dataset/test_Oversample.csv') #write augmented data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(df):\n",
    "    \"\"\"\n",
    "    Randomly undersamples dataset.\n",
    "    df: pandas dataframe\n",
    "    \"\"\" \n",
    "    x = df[df.score == 1]\n",
    "    y = df[df.score == 0]\n",
    "    i = 0\n",
    "    data = pd.DataFrame(data=x)\n",
    "    while (i < len(x)):\n",
    "        rand = randint(0,len(y)-1)\n",
    "        data = data.append(y.iloc[rand]).reset_index(drop=True)\n",
    "        i += 1\n",
    "    res = shuffle(data)\n",
    "    return res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = get_data('data_500_label.csv')\n",
    "df_undersample = undersample(df3)\n",
    "data_distribution(df_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_undersample.to_csv('dataset/test_Undersample.csv') #write augmented data to file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
