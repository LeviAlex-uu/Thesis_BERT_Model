{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\levia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\levia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\levia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def single_dataset(file, dir=\"data/\", lim=0):\n",
    "    \"\"\"\n",
    "    Reads file given, removes unnecessary columns and labels data.\n",
    "    file: name of .csv file to be read\n",
    "    dir: directory of the file \n",
    "    lim: limit of the data entries returned\n",
    "    \"\"\"\n",
    "    with open(dir + file, encoding=\"utf-8\") as f:\n",
    "        lines = [line[:-1] for line in f.readlines()]\n",
    "    header = lines[0].split(',')\n",
    "    clean = len(header) == 2 or file == 'test.csv'\n",
    "    \n",
    "    df = pd.read_csv(dir + file)\n",
    "    if not clean: \n",
    "        for head in header:\n",
    "            if head == 'score' or head == 'caption':\n",
    "                pass\n",
    "            else:\n",
    "                df.pop(head)\n",
    "    if lim > 0:\n",
    "        df.sort_values(by='score', ascending=False)\n",
    "        labeled_df = label_data(df)\n",
    "        return labeled_df[:lim]\n",
    "    else:\n",
    "        labeled_df = label_data(df)\n",
    "        return labeled_df   \n",
    "\n",
    "def complete_dataset():\n",
    "    \"\"\"\n",
    "    combine all excel-files to create dataset\n",
    "    \"\"\"\n",
    "    dir = \"data/\"\n",
    "    ds  = pd.read_csv(\"data/test.csv\")\n",
    "    \n",
    "    for file in os.listdir(dir):\n",
    "        df = single_dataset(file, dir=dir)\n",
    "        ds = pd.concat([ds,df], ignore_index=True)\n",
    "        \n",
    "    ds.to_csv('dataset/data_fullbinary.csv')   \n",
    "    \n",
    "def get_data(file):\n",
    "    \"\"\"\n",
    "    Load dataset from a specified file\n",
    "    file: name of .csv file to be read\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"dataset/\" + file) \n",
    "    return df\n",
    "    \n",
    "def label_data(df):\n",
    "    \"\"\"\n",
    "    label the scores as 'funny' or 'unfunny'\n",
    "    df: pandas dataframe\n",
    "    \"\"\"\n",
    "    df['score'] = df['score'].apply(lambda x: 1 if x >= 2.0 else 0)\n",
    "    df['label'] = pd.DataFrame(df['score'].apply(lambda x: 'funny' if x == 1 else 'unfunny'))\n",
    "    return df \n",
    "    \n",
    "    \n",
    "def data_distribution(df):\n",
    "    \"\"\"\n",
    "    Shows how the data is distributed in funny and unfunny captions\n",
    "    df: pandas dataframe\n",
    "    \"\"\"\n",
    "    num = df['label'].value_counts()\n",
    "    if \"funny\" in num:\n",
    "        fun = round(num['funny']/len(df)*100, 2)\n",
    "        print(\"Amount of funny captions:\\t\\t\", num['funny'], \"\\t\", fun, \"%\")\n",
    "        \n",
    "    if \"unfunny\" in num:\n",
    "        nfun = round(num['unfunny']/len(df)*100, 2)\n",
    "        print(\"Amount of unfunny captions:\\t\\t\", num['unfunny'], \"\\t\", nfun, \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I check, “exceeded expectations?”</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Let’s just give him the damn cheese.</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hmmmmm, we didn’t count on his thinking above ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>'What's more impressive is he's already filed ...</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                            caption  label\n",
       "0      1           Should I check, “exceeded expectations?”  funny\n",
       "1      1               Let’s just give him the damn cheese.  funny\n",
       "2      1  Hmmmmm, we didn’t count on his thinking above ...  funny\n",
       "3      1  'What's more impressive is he's already filed ...  funny"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = single_dataset('691_summary_KLUCB.csv')\n",
    "df[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of funny captions:\t\t 5 \t 0.14 %\n",
      "Amount of unfunny captions:\t\t 3655 \t 99.86 %\n"
     ]
    }
   ],
   "source": [
    "data_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\levia\\anaconda3\\envs\\BERT\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def augmentMyData(df, augmenter, repetitions=1, samples=200):\n",
    "    \"\"\"\n",
    "    AugmentMyData is the implementation of the SMOTE technique, \n",
    "    it creates synthetic data points using the bert-base-uncased word embedder as augmenter.\n",
    "    df: pandas dataframe\n",
    "    augmenter: augmenter, contextual word embedder\n",
    "    repetitions: times the augmentation process is repeated\n",
    "    samples: amount of data points created in one repetition.\n",
    "    This function is written using code found here:\n",
    "    https://github.com/theartificialguy/NLP-with-Deep-Learning/blob/master/PREPROCESSING%20TECHNIQUES/spam_handling_imbalanced_data.ipynb\n",
    "    \"\"\"\n",
    "    augmented_texts = []\n",
    "    # select only the minority class samples\n",
    "    spam_df = df[df['score'] == 1].reset_index(drop=True) # removes unecessary index column\n",
    "    for i in tqdm(np.random.randint(0, len(spam_df), samples)):\n",
    "        # generating 'n_samples' augmented texts\n",
    "        for _ in range(repetitions):\n",
    "            augmented_text = augmenter.augment(spam_df['caption'].iloc[i])\n",
    "            augmented_texts.append(augmented_text)\n",
    "    \n",
    "    data = {\n",
    "        'score': 1,\n",
    "        'caption': augmented_texts,\n",
    "        'label': 'funny'\n",
    "    }\n",
    "    aug_df = pd.DataFrame(data)\n",
    "    df = shuffle(df.append(aug_df).reset_index(drop=True))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#print(torch. __version__) \n",
    "import nlpaug.augmenter.word.context_word_embs as aug\n",
    "augmenter = aug.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\") #BERT-base-uncased word embedding augmenter\n",
    "\n",
    "df2 = get_data('data_500_label.csv')\n",
    "sample = df2.sample(frac=0.1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (len(sample[sample['score'] == 0]) - len(sample[sample['score'] == 1]))/4\n",
    "df2_aug = augmentMyData(sample, augmenter, repititions=4, samples=diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_aug.to_csv('dataset/test_SMOTE.csv') #write augmented data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def oversample(df):\n",
    "    \"\"\"\n",
    "    randomly oversamples dataset.\n",
    "    df: pandas dataframe\n",
    "    \"\"\"\n",
    "    x = df[df.score == 1]\n",
    "    y = df[df.score == 0]\n",
    "    i = len(x)\n",
    "    \n",
    "    while (i < len(y)):\n",
    "        rand = randint(0,len(x)-1)\n",
    "        df = df.append(x.iloc[rand]).reset_index(drop=True)\n",
    "        i += 1\n",
    "    res = shuffle(df)\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of funny captions:\t\t 9014 \t 50.0 %\n",
      "Amount of unfunny captions:\t\t 9014 \t 50.0 %\n"
     ]
    }
   ],
   "source": [
    "df_oversample = oversample(sample)\n",
    "data_distribution(df_oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oversample.to_csv('dataset/test_Oversample.csv') #write augmented data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(df):\n",
    "    \"\"\"\n",
    "    Randomly undersamples dataset.\n",
    "    df: pandas dataframe\n",
    "    \"\"\" \n",
    "    x = df[df.score == 1]\n",
    "    y = df[df.score == 0]\n",
    "    i = 0\n",
    "    data = pd.DataFrame(data=x)\n",
    "    while (i < len(x)):\n",
    "        rand = randint(0,len(y)-1)\n",
    "        data = data.append(y.iloc[rand]).reset_index(drop=True)\n",
    "        i += 1\n",
    "    res = shuffle(data)\n",
    "    return res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of funny captions:\t\t 391 \t 50.0 %\n",
      "Amount of unfunny captions:\t\t 391 \t 50.0 %\n"
     ]
    }
   ],
   "source": [
    "df3 = get_data('data_500_label.csv')\n",
    "df_undersample = undersample(df3)\n",
    "data_distribution(df_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_undersample.to_csv('dataset/test_Undersample.csv') #write augmented data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
